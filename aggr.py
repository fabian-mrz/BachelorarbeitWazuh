import os
import json
import time
import requests
import pandas as pd
from datetime import datetime
from typing import List, Dict, Generator
from logger import logger
import gc
import ijson
from configparser import ConfigParser


# Constants
EVENTS_DIR = './events'
CSV_STORAGE_DIR = './static/incidents'
INCIDENT_SERVER_URL = "https://192.168.178.40:8334/incidents/"
RULE_DELAY = 5
BATCH_SIZE = 50  # Reduced batch size
MAX_BUFFER_SIZE = 100  # Limit buffer size
CLEANUP_INTERVAL = 60  # Cleanup every minute
FILE_CHECK_INTERVAL = 5  # Check every 5 seconds
# Get path to cert relative to script location
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CERT_PATH = os.path.join(SCRIPT_DIR, 'cert.pem')




def get_default_token():
    config = ConfigParser()
    try:
        config.read('config.ini')
        if 'API' in config and 'tokens' in config['API']:
            token_pairs = config['API']['tokens'].split(',')
            for pair in token_pairs:
                name, token = pair.split(':')
                if name.strip() == 'default':
                    return token.strip()
        raise ValueError("Default token not found in config.ini")
    except Exception as e:
        print(f"Error reading config: {str(e)}")
        return None
    
token = get_default_token() or "1234"  # Fallback to "1234" if config read fails

# Setup
os.makedirs(CSV_STORAGE_DIR, exist_ok=True)

class EventProcessor:
    def __init__(self):
        self.last_rule_events = {}
        self.events_buffer = {}
        self.last_cleanup = time.time()

    def get_rule_id_from_filename(self, filename: str) -> str:
        """Extract rule ID from filename"""
        return filename.split('_')[1].split('.')[0]
    
    def stream_json_events(self, filepath: str) -> Generator[Dict, None, None]:
        """Stream JSON events instead of loading entire file"""
        try:
            with open(filepath, 'rb') as f:
                parser = ijson.items(f, 'item')
                for event in parser:
                    yield event
        except (OSError, ijson.JSONError) as e:
            logger.error(f"Error streaming JSON events from {filepath}: {e}")
            raise

    def save_events_to_csv(self, events: List[Dict], rule_id: str) -> str:
        """Save events to CSV file with chunking"""
        incident_id = f"RULE_{rule_id}_{int(time.time())}"
        directory = os.path.join(CSV_STORAGE_DIR, incident_id)
        os.makedirs(directory, exist_ok=True)
        csv_path = os.path.join(directory, 'events.csv')
        
        try:
            # Write CSV in chunks
            for i in range(0, len(events), 1000):
                chunk = events[i:i + 1000]
                mode = 'w' if i == 0 else 'a'
                pd.DataFrame(chunk).to_csv(csv_path, index=False, mode=mode, header=(i == 0))
        except (OSError, pd.errors.EmptyDataError) as e:
            logger.error(f"Error saving events to CSV for rule {rule_id}: {e}")
            raise
        
        return f'/incidents/{incident_id}/events.csv'
    
    def cleanup_buffers(self):
        """Perform periodic cleanup"""
        current_time = time.time()
        if current_time - self.last_cleanup > CLEANUP_INTERVAL:
            for rule_id in list(self.events_buffer.keys()):
                if len(self.events_buffer[rule_id]) > MAX_BUFFER_SIZE:
                    self.events_buffer[rule_id] = self.events_buffer[rule_id][-MAX_BUFFER_SIZE:]
            self.last_cleanup = current_time
            gc.collect()  # Force garbage collection

    def create_incident(self, events: List[Dict], rule_id: str, csv_path: str):
        """Create incident from events"""
        now = datetime.now()
        first_event = events[0]
        
        # Determine severity based on rule level
        severity = "high" if int(first_event['rule']['level']) >= 10 else "medium"
        
        incident = {
            "id": f"RULE_{rule_id}_{int(now.timestamp())}",
            "title": first_event['rule']['description'],
            "severity": severity,
            "source": "wazuh",
            "description": json.dumps({
                "rule_id": rule_id,
                "rule_description": first_event['rule']['description'],
                "rule_level": first_event['rule']['level'],
                "total_events": len(events),
                "agent_names": list(set(e['agent']['name'] for e in events)),
                "first_event_timestamp": events[0]['timestamp'],
                "last_event_timestamp": events[-1]['timestamp'],
                "sample_event": first_event,
                "csv_path": csv_path
            }),
            "acknowledged": False,
            "escalated": False,
            "created_at": now.isoformat()
        }

        try:
            response = requests.post(
                INCIDENT_SERVER_URL,
                json=incident,
                headers={
                    "Content-Type": "application/json",
                    "X-API-Key": token
                },
                verify=CERT_PATH if os.path.exists(CERT_PATH) else False
            )
            response.raise_for_status()
            logger.info(f"‚úÖ Created incident for rule {rule_id}")
        except requests.exceptions.SSLError as e:
            logger.error(f"SSL verification failed: {str(e)}")
            raise

    def process_file(self, filepath: str, rule_id: str):
        """Process file in streaming mode"""
        try:
            self.cleanup_buffers()
            current_batch = []
            
            for event in self.stream_json_events(filepath):
                current_batch.append(event)
                
                if len(current_batch) >= BATCH_SIZE:
                    csv_path = self.save_events_to_csv(current_batch, rule_id)
                    self.create_incident(current_batch, rule_id, csv_path)
                    current_batch = []
                    gc.collect()
            
            if current_batch:  # Process remaining events
                csv_path = self.save_events_to_csv(current_batch, rule_id)
                self.create_incident(current_batch, rule_id, csv_path)
            
            if os.path.exists(filepath):
                os.remove(filepath)
                
        except Exception as e:
            logger.error(f"Error processing {filepath}: {e}")
        finally:
            gc.collect()

    def monitor_directory(self):
        """Monitor directory for new event files"""
        logger.info(f"üîç Monitoring {EVENTS_DIR} for rule events...")
        
        while True:
            try:
                for filename in os.listdir(EVENTS_DIR):
                    if not filename.endswith('.json'):
                        continue
                        
                    filepath = os.path.join(EVENTS_DIR, filename)
                    rule_id = self.get_rule_id_from_filename(filename)
                    self.process_file(filepath, rule_id)
                    
            except Exception as e:
                logger.error(f"Directory monitoring error: {e}")
                
            time.sleep(FILE_CHECK_INTERVAL)

def main():
    processor = EventProcessor()
    try:
        processor.monitor_directory()
    except KeyboardInterrupt:
        logger.info("‚ö° Stopping event monitoring...")

if __name__ == "__main__":
    main()